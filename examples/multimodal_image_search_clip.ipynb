{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238520e0",
   "metadata": {},
   "source": [
    "# Multimodal Search Using CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3590f0e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how SuperDuperDB can perform searches involving both text and images using the [CLIP multimodal architecture](https://openai.com/research/clip). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we start, make sure you have the necessary tools by running these commands:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40272d6a2681c8e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install superduperdb\n",
    "!pip install ipython openai-clip\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f94ae8",
   "metadata": {},
   "source": [
    "## Connect to datastore \n",
    "\n",
    "Connect to a MongoDB datastore using SuperDuperDB. Adjust the connection URI based on your setup.\n",
    "Here are some examples of `MongoDB URIs`:\n",
    "\n",
    "* For testing (default connection): `mongomock://test`\n",
    "* Local MongoDB instance: `mongodb://localhost:27017`\n",
    "* MongoDB with authentication: `mongodb://superduper:superduper@mongodb:27017/documents`\n",
    "* MongoDB Atlas: `mongodb+srv://<username>:<password>@<atlas_cluster>/<database>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ef986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from superduperdb import superduper\n",
    "from superduperdb.backends.mongodb import Collection\n",
    "\n",
    "# SuperDuper your Database!\n",
    "mongodb_uri = os.getenv(\"MONGODB_URI\", \"mongomock://test\")\n",
    "db = superduper(mongodb_uri, artifact_store='filesystem://.data')\n",
    "\n",
    "collection = Collection('multimodal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6d6b0",
   "metadata": {},
   "source": [
    "## Load Dataset \n",
    "\n",
    "For simplicity, we'll work with a subset of the [Tiny-Imagenet dataset](https://paperswithcode.com/dataset/tiny-imagenet). To insert images into the database, we utilize the `Encoder`-`Document` framework, which allows saving Python class instances as blobs in the `Datalayer` and retrieving them as Python objects. We will use the `PIL.Image` encoders, but it's also possible to create your own encoders for your custom data types."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download images locally."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b348f9f620dc02c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f14fb-8e79-4bc6-88af-1a800aecb8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://superduperdb-public.s3.eu-west-1.amazonaws.com/coco_sample.zip\n",
    "!unzip -f coco_sample.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert images to Python objects."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f27ecc93b5f9c87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from superduperdb import Document\n",
    "from superduperdb.ext.pillow import pil_image as i\n",
    "import glob\n",
    "\n",
    "images = glob.glob('images_small/*.jpg')\n",
    "documents = [Document({'image': i(uri=f'file://{img}')}) for img in images][:500]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3254b8b87807612"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ensure that images are represented correctly as Python objects."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ac495d1606b475d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a63bf-9e1f-4266-823a-7a2208937e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c7e282",
   "metadata": {},
   "source": [
    "The wrapped python objects may be inserted directly to the `Datalayer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(collection.insert_many(documents), encoders=(i,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d37264",
   "metadata": {},
   "source": [
    "Verify that the images are stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = db.execute(collection.find_one()).unpack()['image']\n",
    "display(x.resize((300, 300 * (1+int(x.size[1] / x.size[0])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab27b50",
   "metadata": {},
   "source": [
    "## Build Models\n",
    "\n",
    "Now, let's prepare the CLIP model for multimodal search, which involves two components: `text encoding` and `visual encoding`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916792d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from superduperdb import vector\n",
    "from superduperdb.ext.torch import TorchModel\n",
    "\n",
    "# Load the CLIP model\n",
    "model, preprocess = clip.load(\"RN50\", device='cpu')\n",
    "\n",
    "# Define a vector\n",
    "e = vector(shape=(1024,))\n",
    "\n",
    "# Create a TorchModel for text encoding\n",
    "text_model = TorchModel(\n",
    "    identifier='clip_text',\n",
    "    object=model,\n",
    "    preprocess=lambda x: clip.tokenize(x)[0],\n",
    "    postprocess=lambda x: x.tolist(),\n",
    "    encoder=e,\n",
    "    forward_method='encode_text',    \n",
    ")\n",
    "\n",
    "# Create a TorchModel for visual encoding\n",
    "visual_model = TorchModel(\n",
    "    identifier='clip_image',\n",
    "    object=model.visual,    \n",
    "    preprocess=preprocess,\n",
    "    postprocess=lambda x: x.tolist(),\n",
    "    encoder=e,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716bcb2",
   "metadata": {},
   "source": [
    "## Create a Vector-Search Index\n",
    "\n",
    "Now, let's make a system that can search for both text and images using vectors. \n",
    "\n",
    "We'll add both the `visual_model` and the `text_model` to the search system, but they have different roles. \n",
    "\n",
    "The `visual_model` will be designated as the primary transformer (`indexing_listener`), in charge of creating vectors in the database.\n",
    "The `text_model` will serve as the secondary transformer (`compatible_listener`), defining how an alternative model can search for those vectors.\n",
    "This way, we can use different models for searching, even if they expect different types of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb import VectorIndex\n",
    "from superduperdb import Listener\n",
    "\n",
    "# Create a VectorIndex and add it to the database\n",
    "db.add(\n",
    "    VectorIndex(\n",
    "        'my-index',\n",
    "        indexing_listener=Listener(\n",
    "            model=visual_model,\n",
    "            key='image',\n",
    "            select=collection.find(),\n",
    "            predict_kwargs={'batch_size': 10},\n",
    "        ),\n",
    "        compatible_listener=Listener(\n",
    "            model=text_model,\n",
    "            key='text',\n",
    "            active=False,\n",
    "            select=None,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18971a6d",
   "metadata": {},
   "source": [
    "## Search Images Using Text\n",
    "\n",
    "Now we can demonstrate searching for images using text queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab994b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from superduperdb import Document\n",
    "\n",
    "query_string = 'sports'\n",
    "\n",
    "search_results = db.execute(\n",
    "    collection.like(\n",
    "        Document({'text': query_string}), # Search image by text\n",
    "        vector_index='my-index', \n",
    "        n=3,\n",
    "    ).find({})\n",
    ")\n",
    "\n",
    "# Display the images from the search results\n",
    "for r in search_results:\n",
    "    x = r['image'].x\n",
    "    display(x.resize((300, int(300 * x.size[1] / x.size[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Search For Similar Images\n",
    "\n",
    "Besides searching for images using text, we can use the same vectors to search for similar images.  \n",
    "\n",
    "To do so, let's pick a random image, and use it as a reference to finding the similar ones.   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8af187f9591bcf6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e3ac22-044f-4675-976a-68ff9b59efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickup a random image as reference.\n",
    "ref_img = db.execute(collection.find_one({}))['image']\n",
    "x = ref_img.x\n",
    "display(x.resize((300, int(300 * x.size[1] / x.size[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The process is now the same before:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebb9c3749ef0cb72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8569e4f-74f2-4ee5-9674-7829b2fcc62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = db.execute(\n",
    "    collection.like(\n",
    "        Document({'image': ref_img}), # Search similar images\n",
    "        vector_index='my-index', \n",
    "        n=3,\n",
    "    ).find({})\n",
    ")\n",
    "\n",
    "for r in cur:\n",
    "    x = r['image'].x\n",
    "    display(x.resize((300, int(300 * x.size[1] / x.size[0]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
