{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be320b36",
   "metadata": {},
   "source": [
    "# Postgres + Pgvector (HNSW and IVFflat Indexing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6841b7",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before starting the implementation, make sure you have the required libraries installed by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d47e1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install superduperdb\n",
    "# !pip install vllm\n",
    "# !pip install sentence_transformers numpy==1.24.4\n",
    "# !pip install 'ibis-framework[postgres]'\n",
    "# !pip install pgvector\n",
    "# !pip install psycopg2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b8392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf .superduperdb/ && mkdir -p .superduperdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44fba27",
   "metadata": {},
   "source": [
    "## Connect to datastore \n",
    "\n",
    "First, we need to establish a connection to a Postgres datastore via SuperDuperDB. You can configure the `Postgres_URI` based on your specific setup. \n",
    "Here are some examples of postgres URIs:\n",
    "\n",
    "* For testing (default connection): `postgres://test`\n",
    "* Local postgres instance: `postgres://localhost:27017`\n",
    "* postgres with authentication: `postgres://superduper:superduper@postgres:27017/documents`\n",
    "* postgres Atlas: `postgres+srv://<username>:<password>@<atlas_cluster>/<database>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.base.config import VectorSearch, Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9fb3c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from superduperdb import superduper\n",
    "from superduperdb.backends.ibis import Table\n",
    "import os\n",
    "from superduperdb.backends.ibis.field_types import dtype\n",
    "from superduperdb.ext.pillow import pil_image\n",
    "from superduperdb import Schema\n",
    "\n",
    "connection_uri = \"postgresql://postgres:test@localhost:8000/qa\"\n",
    "\n",
    "\n",
    "# It just super dupers your database\n",
    "db = superduper(\n",
    "    connection_uri,\n",
    "    metadata_store='sqlite:///.superduperdb/metadata.sqlite',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f93d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m superduperdb info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "ROOT = '../docs/hr/content/docs/'\n",
    "\n",
    "STRIDE = 3       # stride in numbers of lines\n",
    "WINDOW = 25       # length of window in numbers of lines\n",
    "\n",
    "files = sorted(glob.glob(f'{ROOT}/**/*.md', recursive=True))\n",
    "\n",
    "def get_chunk_link(chunk, file_name):\n",
    "    # Get the original link of the chunk\n",
    "    file_link = file_name[:-3].replace(ROOT, 'https://docs.superduperdb.com/docs/docs/')\n",
    "    # If the chunk has subtitles, the link to the first subtitle will be used first.\n",
    "    first_title = (re.findall(r'(^|\\n)## (.*?)\\n', chunk) or [(None, None)])[0][1]\n",
    "    if first_title:\n",
    "        # Convert subtitles and splice URLs\n",
    "        first_title = first_title.lower()\n",
    "        first_title = re.sub(r'[^a-zA-Z0-9]', '-', first_title)\n",
    "        file_link = file_link + '#' + first_title\n",
    "    return file_link\n",
    "\n",
    "def create_chunk_and_links(file, file_prefix=ROOT):\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    if len(lines) > WINDOW:\n",
    "        chunks = ['\\n'.join(lines[i: i + WINDOW]) for i in range(0, len(lines), STRIDE)]\n",
    "    else:\n",
    "        chunks = ['\\n'.join(lines)]\n",
    "    return [{'txt': chunk, 'link': get_chunk_link(chunk, file)}  for chunk in chunks]\n",
    "\n",
    "\n",
    "all_chunks_and_links = sum([create_chunk_and_links(file) for file in files], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72785e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use !curl to download the 'superduperdb_docs.json' file\n",
    "!curl -O https://datas-public.s3.amazonaws.com/superduperdb_docs.json\n",
    "\n",
    "import json\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Open the downloaded JSON file and load its contents into the 'chunks' variable\n",
    "with open('superduperdb_docs.json') as f:\n",
    "    all_chunks_and_links = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afba211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks_and_links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd41263",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_chunks_and_links = list()\n",
    "for i, e in enumerate(all_chunks_and_links):\n",
    "    e['id'] = i\n",
    "    new_all_chunks_and_links.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97113997",
   "metadata": {},
   "source": [
    "## Define Schema and Create table\n",
    "\n",
    "For this use-case, you need a table with images and another table with text. SuperDuperDB extends standard SQL functionality, allowing developers to define their own data types through the `Encoder` abstraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "Schema(\n",
    "        'questiondocs-schema',\n",
    "        fields={'id': dtype(str), 'txt': dtype(str), 'link': dtype(str)},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35365286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Define the 'captions' table\n",
    "table = Table(\n",
    "    'questiondocs',\n",
    "    primary_id='id',\n",
    "    schema=Schema(\n",
    "        'questiondocs-schema',\n",
    "        fields={'id': dtype(str), 'txt': dtype(str), 'link': dtype(str)},\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Add the 'captions' and 'images' tables to the SuperDuperDB database\n",
    "db.add(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d34dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9bbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_chunks_and_links_df = pd.DataFrame(new_all_chunks_and_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca8f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_all_chunks_and_links_df.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8596cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.base.document import Document as D\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b67d92cf",
   "metadata": {},
   "source": [
    "table.insert(df[['id', 'txt', 'link']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d1595",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert = table.insert(\n",
    "        [\n",
    "            D(\n",
    "                {\n",
    "                    'id': d['id'],\n",
    "                    'txt': d['txt'],\n",
    "                    'link': d['link'],\n",
    "                }\n",
    "            )\n",
    "            for i, d in df.iterrows()\n",
    "        ]\n",
    "    )\n",
    "_ =  db.execute(insert)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5aeb139c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "_ = db.execute(table.insert(df[['id', 'txt', 'link']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee273e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = table.select('txt', 'link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097b7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = db.execute(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48b0b3",
   "metadata": {},
   "source": [
    "A `Model` is a wrapper around a self-built or ecosystem model, such as `torch`, `transformers`, `openai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f428a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from superduperdb import vector\n",
    "vector(shape=(1024,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbefeae8",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "from superduperdb.ext.sentence_transformers import SentenceTransformer\n",
    "from superduperdb.ext.numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db75570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\n",
    "    identifier=\"embedding\",\n",
    "    object=sentence_transformers.SentenceTransformer(\"BAAI/bge-large-en-v1.5\"),\n",
    "    postprocess=lambda x: x.tolist(),\n",
    "    datatype=vector(shape=(1024,)),\n",
    "    predict_kwargs={\"show_progress_bar\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef3fb79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector = model.predict_one('This is a test')\n",
    "print('vector size: ', len(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c8134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Listener class from the superduperdb module\n",
    "from superduperdb import Listener\n",
    "\n",
    "\n",
    "# Create a Listener instance with the specified model, key, and selection criteria\n",
    "listener1 = Listener(\n",
    "    model=model,          # The model to be used for listening\n",
    "    key='txt',            # The key field in the documents to be processed by the model\n",
    "    select=table.select('id', 'txt'),  # The selection criteria for the documents\n",
    "    predict_kwargs={'max_chunk_size': 3000},\n",
    "    identifier='listener1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874921eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db.add(listener1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a2742",
   "metadata": {},
   "source": [
    "## HNSW (Hierarchical Navigable Small Worlds graph) Indexing\n",
    "\n",
    "1. HNSW Indexing - Multi layer graph structure\n",
    "2. IVFFlat Indexing - is based on clustering\n",
    "\n",
    "\n",
    "> Note : `indexing_measure` and `measure` both should use same similarity approaches. Otherwise it will go for sequential scanning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bdc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb import VectorIndex\n",
    "from superduperdb.vector_search.postgres import PostgresVectorSearcher, HNSW, IVFFlat\n",
    "\n",
    "hnsw_indexing = HNSW(m=16, ef_construction=64, ef_search=49)\n",
    "ivfflat_indexing = IVFFlat(lists=100, probes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0056bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = VectorIndex(\n",
    "    identifier='my-index',        # Unique identifier for the VectorIndex\n",
    "    indexing_listener=listener1,    # Listener to be used for indexing documents\n",
    "    measure='cosine',\n",
    "    indexing = hnsw_indexing,\n",
    "    indexing_measure = 'vector_cosine_ops'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93636671",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jobs, _ = db.add(vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3293b8",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1365988",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from superduperdb.backends.ibis import Table\n",
    "from superduperdb import Document as D\n",
    "from IPython.display import *\n",
    "\n",
    "# Define the query for the search\n",
    "query = 'Code snippet how to create a `VectorIndex` with a torchvision model'\n",
    "# query = 'can you explain vector-indexes with `superduperdb`?'\n",
    "\n",
    "# Execute a search using SuperDuperDB to find documents containing the specified query\n",
    "result = db.execute(\n",
    "    query=table.like(D({'txt': query}), vector_index='my-index', n=5).select('id', 'txt', 'link')\n",
    ")\n",
    "\n",
    "# Display a horizontal rule to separate results\n",
    "display(Markdown('---'))\n",
    "\n",
    "# Display each document's 'txt' field and separate them with a horizontal rule\n",
    "for r in result:\n",
    "    display(Markdown(r['txt']))\n",
    "    display(r['link'])\n",
    "    display(Markdown('---'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a3c179",
   "metadata": {},
   "source": [
    "## Future Works\n",
    "1. `Ibis` doesn't support `pgvector`. and want to make it supportable for that `pgvector`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
