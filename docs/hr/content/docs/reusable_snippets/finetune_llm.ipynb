{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a88963f-a129-4848-9daa-ba025450d190",
   "metadata": {},
   "source": [
    "<!-- TABS -->\n",
    "# Finetune LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af45ab4-aad0-4f71-83ed-5ec0df038382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <testing: >\n",
    "# !pip install trl datasets transformers bitsandbytes peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b125f-8b8e-4bc8-b4aa-8366d46ef1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <testing: >\n",
    "import os\n",
    "os.environ[\"SUPERDUPERDB_DATA_BACKEND\"] = 'mongodb://localhost:27017/llm'\n",
    "os.environ[\"SUPERDUPERDB_ARTIFACT_STORE\"] = \"filesystem://./outoput/artifact_store\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b7676-c14e-41a3-95f3-1654d1b6faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <testing: >\n",
    "from superduperdb import superduper\n",
    "from superduperdb.backends.mongodb import Collection\n",
    "from superduperdb.base.document import Document\n",
    "\n",
    "db = superduper(os.environ.get(\"SUPERDUPERDB_DATA_BACKEND\", \"mongomock://test\"))\n",
    "db.drop(True)\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name = \"facebook/opt-350m\"\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "train_documents = [\n",
    "    Document({\"text\": example[\"text\"], \"_fold\": \"train\"})\n",
    "    for example in train_dataset\n",
    "]\n",
    "eval_documents = [\n",
    "    Document({\"text\": example[\"text\"], \"_fold\": \"valid\"})\n",
    "    for example in eval_dataset\n",
    "]\n",
    "\n",
    "db.execute(Collection(\"datas\").insert_many(train_documents[:100]))\n",
    "db.execute(Collection(\"datas\").insert_many(eval_documents[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83586f68-c684-4362-a837-0aba038a8f27",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8cc68-dd48-41f8-9e61-9e765746d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Local>\n",
    "from superduperdb.ext.transformers import LLM, LLMTrainer\n",
    "trainer = LLMTrainer(\n",
    "    identifier=\"llm-finetune-trainer\",\n",
    "    output_dir=\"output/finetune\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_seq_length=512,\n",
    "    use_lora=True, \n",
    "    bits=4,\n",
    ")\n",
    "\n",
    "llm = LLM(\n",
    "    identifier=\"llm\",\n",
    "    model_name_or_path=model_name,\n",
    "    trainer=trainer,\n",
    "    train_X=\"text\", # Which field of data is used for training\n",
    "    train_select=Collection('datas').find() # Which table data to use for training\n",
    ")\n",
    "\n",
    "llm.fit_in_db(db=db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585f644-4a0e-4a57-97b0-b9f575206c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: On Ray>\n",
    "from superduperdb.ext.transformers import LLM, LLMTrainer\n",
    "trainer = LLMTrainer(\n",
    "    identifier=\"llm-finetune-trainer\",\n",
    "    output_dir=\"output/finetune\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_seq_length=512,\n",
    "    use_lora=True,\n",
    "    bits=4,\n",
    "    ray_address=\"ray://localhost:10001\", # set ray_address\n",
    ")\n",
    "\n",
    "llm = LLM(\n",
    "    identifier=\"llm\",\n",
    "    model_name_or_path=model_name,\n",
    "    trainer=trainer,\n",
    "    train_X=\"text\", # Which field of data is used for training\n",
    "    train_select=Collection('datas').find() # Which table data to use for training\n",
    ")\n",
    "\n",
    "llm.fit_in_db(db=db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316adae3-8f51-4855-a3f6-0eb4c20ce90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Deepspeed>\n",
    "# !pip install deepspeed\n",
    "from superduperdb.ext.transformers import LLM, LLMTrainer\n",
    "deepspeed = {\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "    },\n",
    "}\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    identifier=\"llm-finetune-trainer\",\n",
    "    output_dir=\"output/finetune\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_seq_length=512,\n",
    "    use_lora=True,\n",
    "    bits=4,\n",
    "    deepspeed=deepspeed, # set deepspped\n",
    ")\n",
    "\n",
    "llm = LLM(\n",
    "    identifier=\"llm\",\n",
    "    model_name_or_path=model_name,\n",
    "    trainer=trainer,\n",
    "    train_X=\"text\", # Which field of data is used for training\n",
    "    train_select=Collection('datas').find() # Which table data to use for training\n",
    ")\n",
    "\n",
    "llm.fit_in_db(db=db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086fd733-052a-4928-9cc4-028585c20098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Multi-GPUS>\n",
    "from superduperdb.ext.transformers import LLM, LLMTrainer\n",
    "trainer = LLMTrainer(\n",
    "    identifier=\"llm-finetune-trainer\",\n",
    "    output_dir=\"output/finetune\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_seq_length=512,\n",
    "    use_lora=True,\n",
    "    bits=4,\n",
    "    num_gpus=2, # set num_gpus\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543d2e06-8e18-441a-92d1-b785d6d3705e",
   "metadata": {},
   "source": [
    "## Load the trained model\n",
    "There are two methods to load a trained model:\n",
    "\n",
    "- **Load the model directly**: This will load the model with the best metrics (if the transformers' best model save strategy is set) or the last version of the model.\n",
    "- **Use a specified checkpoint**: This method downloads the specified checkpoint, then initializes the base model, and finally merges the checkpoint with the base model. This approach supports custom operations such as resetting flash_attentions, model quantization, etc., during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df18b8ad-35b6-4ecc-813d-f29d3afe795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Load Trained Model Directly>\n",
    "llm = db.load(\"model\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a886bb-2c1b-4aee-815f-f9ef3c0e346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tab: Use a specified checkpoint>\n",
    "from superduperdb.ext.transformers import LLM, LLMTrainer\n",
    "checkpoint = db.load(\"checkpoint\", trainer.experiment_id)\n",
    "llm = LLM(\n",
    "    identifier=\"llm\",\n",
    "    model_name_or_path=model_name,\n",
    "    adapter_id=checkpoint,\n",
    "    model_kwargs=dict(load_in_4bit=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ce03f-9c0c-42c8-a89b-896c00f044ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <testing: >\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the capital of Germany? Explain why thats the case and if it was different in the past?\",\n",
    "    }\n",
    "]\n",
    "print(llm.predict_one(messages, max_new_tokens=200, do_sample=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
