{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238520e0",
   "metadata": {},
   "source": [
    "# Multimodal search with CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3590f0e",
   "metadata": {},
   "source": [
    "In this notebook we show-case SuperDuperDB's functionality for searching with multiple types of data over\n",
    "the same `VectorIndex`. This comes out very naturally, due to the fact that SuperDuperDB allows\n",
    "users and developers to add arbitrary models to SuperDuperDB, and (assuming they output vectors) use\n",
    "these models at search/ inference time, to vectorize diverse queries.\n",
    "\n",
    "To this end, we'll be using the [CLIP multimodal architecture](https://openai.com/research/clip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ebe1497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clip in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (1.0)\n",
      "Requirement already satisfied: ftfy in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from clip) (6.1.1)\n",
      "Requirement already satisfied: regex in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from clip) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from clip) (4.65.0)\n",
      "Requirement already satisfied: torch in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from clip) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from clip) (0.15.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from ftfy->clip) (0.2.6)\n",
      "Requirement already satisfied: filelock in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from torch->clip) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from torch->clip) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from torch->clip) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from torch->clip) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from torch->clip) (3.1.2)\n",
      "Requirement already satisfied: numpy in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from torchvision->clip) (1.25.1)\n",
      "Requirement already satisfied: requests in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from torchvision->clip) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from torchvision->clip) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from jinja2->torch->clip) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from requests->torchvision->clip) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from requests->torchvision->clip) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from requests->torchvision->clip) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from requests->torchvision->clip) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from sympy->torch->clip) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03119a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "168b3ae3",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "\n",
    "https://www.kaggle.com/datasets/vikashrajluhaniwal/fashion-images\n",
    "\n",
    "Collection of ~3000 product images under Apparel and Footwear category. Two gender types Boys and Girls under Apparel, similarly Men and Women under Footwear.\n",
    "Each image is identified by an unique ID(ProductId) like 10054.\n",
    "fashion.csv contains additional details about the products like title, description, category, gender etc.\n",
    "\n",
    "Download the dataset (350Mb) via Kaggle API\n",
    "\n",
    "You need to be registered on kaggle first.\n",
    "Download `kaggle.json`` and put it in the right folder to use kaggle python API package. See instructions in https://www.kaggle.com/docs/api\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1048a0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (1.5.16)\n",
      "Requirement already satisfied: six>=1.10 in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from kaggle) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from kaggle) (4.65.0)\n",
      "Requirement already satisfied: python-slugify in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from kaggle) (8.0.1)\n",
      "Requirement already satisfied: urllib3 in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from kaggle) (1.26.16)\n",
      "Requirement already satisfied: bleach in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from kaggle) (6.0.0)\n",
      "Requirement already satisfied: webencodings in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from requests->kaggle) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/.venv/lib/python3.10/site-packages (from requests->kaggle) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "521cd065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "download_path = Path(\"fashion-images.zip\")\n",
    "out_path = Path(\"fashion_images\")\n",
    "data_path = out_path/'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "805834ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/levkonstantinovskiy/.kaggle/kaggle.json'\n",
      "fashion-images.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d vikashrajluhaniwal/fashion-images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8f61290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(download_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(out_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a1190",
   "metadata": {},
   "source": [
    "The real images in this dataset are quite large -  1080 x 1440 or even bigger.\n",
    "This document is for illustration purposes\n",
    " downsize to 64×64 for faster running of this notebook. You don't need to do it with your real data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37209808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "def resize_image(image_path, base_size=64):\n",
    "    with Image.open(image_path) as img:\n",
    "            width, height = img.size\n",
    "            if height > width:\n",
    "                new_height = base_size\n",
    "                new_width = int(new_height * width / height)\n",
    "            else:\n",
    "                new_width = base_size\n",
    "                new_height = int(new_width * height / width)\n",
    "            img_resized = img.resize((new_width, new_height))\n",
    "            img_resized.save(image_path)\n",
    "\n",
    "# Iterate over all .jpg files in the directory and its subdirectories\n",
    "for image_path in data_path.glob('**/*.jpg'):\n",
    "    resize_image(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f94ae8",
   "metadata": {},
   "source": [
    "Let's read product catalog csv. The images are already on disk so we don't need to load the external Image URL column. Instead we add a field with URI pointing to the local disk storage.\n",
    "\n",
    "TODO: Duncan, is there a way to make URIs and encoders play togehter so that URI is downloaded and encoded? would be nice to demo it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32d19321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(data_path/'fashion.csv', usecols=['ProductId', 'Gender', 'Category', 'SubCategory', 'ProductType',\n",
    "       'Colour', 'Usage', 'ProductTitle', 'Image'])\n",
    "#df['ImageURI'] = df.apply(lambda row: (data_path/row['Category']/row['Gender']/'Images'/'images_with_product_ids'/row['Image']).absolute().as_uri(), axis=1)\n",
    "df['ImagePath'] = df.apply(lambda row: str(data_path/row['Category']/row['Gender']/'Images'/'images_with_product_ids'/row['Image']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b5ef986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pymongo\n",
    "from superduperdb.misc.superduper import superduper\n",
    "from superduperdb.models.torch.wrapper import TorchModel\n",
    "from superduperdb.datalayer.mongodb.query import Collection\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "database_name = 'ecommerce-fashion'\n",
    "\n",
    "client = pymongo.MongoClient()\n",
    "client.drop_database(database_name)\n",
    "pymongo_db = client[database_name]\n",
    "db = superduper(pymongo_db)\n",
    "\n",
    "collection = Collection(name='products')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6d6b0",
   "metadata": {},
   "source": [
    "In order to make this notebook easy to execute an play with, we'll use a sub-sample of the [Tiny-Imagenet\n",
    "dataset](https://paperswithcode.com/dataset/tiny-imagenet). \n",
    "\n",
    "Everything we are doing here generalizes to much larger datasets, with higher resolution images, without\n",
    "further ado. For such use-cases, however, it's advisable to use a machine with a GPU, otherwise they'll \n",
    "be some significant thumb twiddling to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29386a5",
   "metadata": {},
   "source": [
    "To get the images into the database, we use the `Encoder`-`Document` framework. This allows\n",
    "us to save Python class instances as blobs in the `Datalayer`, but retrieve them as Python objects.\n",
    "This makes it far easier to integrate Python AI-models with the datalayer.\n",
    "\n",
    "To this end, SuperDuperDB contains pre-configured support for `PIL.Image` instances. It's also \n",
    "possible to create your own encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaa0a06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:found 0 uris\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<pymongo.results.InsertManyResult at 0x291381ab0>,\n",
       " TaskWorkflow(database=<superduperdb.datalayer.base.datalayer.Datalayer object at 0x28fa982b0>, G=<networkx.classes.digraph.DiGraph object at 0x291381bd0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PIL\n",
    "from superduperdb.core.document import Document as SuperDuperDocument\n",
    "from superduperdb.encoders.pillow.image import pil_image as image_encoder\n",
    "\n",
    "df['img'] = df['ImagePath'].apply(lambda x: image_encoder(PIL.Image.open(x)))\n",
    "sdp_documents =  [SuperDuperDocument(my_dict) for my_dict in df.to_dict(orient='records')]\n",
    "db.execute(collection.insert_many(sdp_documents, encoders=(image_encoder,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c7e282",
   "metadata": {},
   "source": [
    "The wrapped python dictionaries may be inserted directly to the `Datalayer`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d37264",
   "metadata": {},
   "source": [
    "We can verify that the images are correctly stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7282a0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAADADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iiigAooooAKKKKAEriPEeuappevSrG/+iJYtMqImSWBxkn2P6V29c54tj26c1xEj/adhiRkODzg4z26U07GVVNxunaxQ0q/1S702O+jvgWlXc8MsO4K3Qgcgj86vWdzdyv9ok1FZcDbsiUBFPfoTk/U157o9pqUhn23f2WRWEZUy58wKeVwTuHU9u4r0HTLG3srPy4IPJDsXZT1yaVyKTcrXX4h4Z1nUdXuNR+120cdvBOY4XGQTjqCPb1966WqtjEkVqoRQu4ljgdST1q12pvU1hFxVpO7FqhqsUc9qIpVDIzYIPfir9UNR3FI1VSeSTgUiznrLQdKsZHa3hUNu3OWbc35nmm6hr8On30NvIBmTJG35sj+nANReINN1G9s2OnO0d0Mbcg4PP8AnrmuKvPDGs3N9PL/AGbciVCqvJCNiyEgZI459Ce3NI56jcFaET2HTpluNPglTO11yM+lWu1Zegxzw6Nbw3EbRyRjaVY5Ix0571qdqZuthaKKKBhRRRQAUUUUAf/Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=48x64>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = db.execute(collection.find_one())['img'].x\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab27b50",
   "metadata": {},
   "source": [
    "We now can wrap the CLIP model, to ready it for multimodel search. It involves 2 components:\n",
    "\n",
    "- text-encoding\n",
    "- visual-encoding\n",
    "\n",
    "Once we have installed both parts, we will be able to search with both images and text for \n",
    "matching items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "916792d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "\n",
    "from superduperdb.encoders.torch.tensor import tensor as tensor_encoder\n",
    "from superduperdb.models.torch.wrapper import TorchModel\n",
    "\n",
    "my_tensor_encoder = tensor_encoder(torch.float, shape=(512,))\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device='cpu')\n",
    "\n",
    "text_model = TorchModel(\n",
    "    identifier='clip_text',\n",
    "    object=model,\n",
    "    preprocess=lambda x: clip.tokenize(x)[0],\n",
    "    forward_method='encode_text',\n",
    "    encoder=my_tensor_encoder\n",
    ")\n",
    "\n",
    "visual_model = TorchModel(\n",
    "    identifier='clip_image',\n",
    "    preprocess=preprocess,\n",
    "    object=model.visual,\n",
    "    encoder=my_tensor_encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6140d056",
   "metadata": {},
   "source": [
    "Let's verify this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ae02683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5430e-01,  8.3420e-02, -8.6218e-02, -2.4812e-01, -9.0192e-02,\n",
       "        -1.7874e-01,  7.9648e-02, -1.4377e+00,  9.5242e-02,  2.9510e-01,\n",
       "         4.2067e-03,  2.5638e-01, -2.1602e-02,  1.5736e-02,  1.9318e-01,\n",
       "        -1.6119e-01,  3.6999e-01,  2.2594e-02, -5.7204e-02, -1.9817e-01,\n",
       "        -7.5303e-03, -3.2254e-01,  2.7348e-01, -3.5396e-01, -1.9037e-01,\n",
       "         1.0515e-01, -2.4183e-02,  1.0074e-01, -8.8070e-02,  2.6366e-02,\n",
       "        -2.7967e-01, -2.9460e-01,  2.1931e-02,  1.1821e-01,  8.7240e-02,\n",
       "         1.1945e-01, -6.0588e-04, -1.0101e-01, -1.0430e-01, -2.0626e-01,\n",
       "        -1.3790e-01, -3.9173e-02,  7.5542e-03,  1.3866e-01, -1.1169e-01,\n",
       "         1.1691e-01, -1.8521e-01,  2.9997e-02,  1.0280e-01,  1.1497e-01,\n",
       "        -1.0389e-01,  5.0075e-02,  3.1277e-01, -1.7357e-01,  4.6908e-02,\n",
       "        -9.6960e-02,  7.7737e-02,  4.9119e-03, -2.9044e-01, -3.6214e-02,\n",
       "         3.3161e-01,  4.0403e-02, -3.6718e-02,  3.3114e-02,  1.3841e-01,\n",
       "         1.1665e-01, -1.8533e-01,  1.4537e-01,  1.7915e-01,  6.0204e-02,\n",
       "         1.8797e-01, -4.3545e-02, -2.5758e-01,  2.5922e-01, -3.2504e-01,\n",
       "        -7.3699e-02, -1.3032e-01, -2.1270e-02, -9.7008e-02, -1.5535e-01,\n",
       "        -1.8003e-01, -2.3091e-01, -7.8064e-02,  2.2982e-01,  3.5198e-02,\n",
       "         2.6405e-01,  1.8695e-01, -2.9914e-01,  4.2317e-01, -2.0544e-03,\n",
       "         1.1283e-01, -1.5321e-02, -2.2041e+00,  3.1780e-01,  1.7714e-01,\n",
       "         1.4387e-01, -2.9229e-01,  7.1943e-02,  1.3914e-01, -2.8608e-01,\n",
       "         1.8457e-01, -1.0049e-01,  9.6526e-02,  2.8571e-01,  6.4735e-02,\n",
       "         2.3304e-02, -3.9703e-02, -7.9729e-02, -8.2036e-02,  2.5785e-02,\n",
       "         1.9965e-02,  4.4991e-01,  2.9948e-02, -1.7797e-02, -1.7955e-02,\n",
       "        -6.9631e-02,  2.1176e-01, -2.4720e-01, -2.2326e-01,  1.1205e-03,\n",
       "         1.1491e-01,  2.9441e-02, -1.3374e-01, -1.6872e-01,  1.1022e-01,\n",
       "        -1.3817e-01,  1.2030e-01,  3.4527e-01,  8.1909e-02,  2.3871e-01,\n",
       "         2.2229e-01, -1.8966e-01, -2.3539e-01,  8.6902e+00,  1.9549e-01,\n",
       "         2.1512e-01, -4.9882e-02, -1.1429e-01, -2.2483e-01,  9.4592e-02,\n",
       "        -7.6710e-02, -9.9087e-02, -6.9110e-02, -1.3233e-01, -3.2581e-01,\n",
       "         2.7050e-02, -2.4327e-01,  1.1758e-01, -1.4014e-01,  1.9817e-01,\n",
       "         8.2304e-02, -1.7757e-01, -3.3039e-02, -9.2580e-02,  2.0600e-01,\n",
       "        -1.3096e-01, -1.4063e-01,  8.7542e-02, -3.2586e-01,  1.9357e-01,\n",
       "        -2.0348e-01, -3.4610e-01, -7.7105e-02, -4.6517e-02, -2.5897e-01,\n",
       "         4.9247e-02,  2.1851e-01, -1.3462e-01,  8.5324e-02,  1.0842e-01,\n",
       "         1.2020e-01, -4.1831e-02,  9.5541e-02,  2.7892e-01, -2.4964e-01,\n",
       "         9.1717e-02, -1.4164e-01,  3.3394e-01,  1.2548e-01,  4.3229e-02,\n",
       "        -7.5975e-02,  1.6848e-01, -6.9535e-02,  1.2126e-01, -1.4141e-01,\n",
       "         1.1193e-01,  1.5343e-01, -9.1191e-03, -3.5644e-01, -4.9832e-02,\n",
       "         2.1788e-02, -2.5872e-01,  1.0919e-01,  2.7175e-03,  1.9120e-01,\n",
       "        -1.4926e-01,  3.6027e-02, -1.0323e-01, -2.1820e-01,  6.5567e-02,\n",
       "         3.0641e-02, -3.5111e-01, -1.7972e-01,  1.1709e-01, -1.1321e-01,\n",
       "        -1.9422e-01, -1.3823e-02, -1.1502e-01,  1.2422e-01,  2.2444e-02,\n",
       "         1.4535e-01,  1.8312e-01, -8.5593e-02, -2.4161e-01, -2.3021e-01,\n",
       "         9.0205e-02, -3.6267e-01, -7.6289e-02,  2.8521e-01,  6.8044e-02,\n",
       "        -1.6620e-02, -1.3670e-01, -5.1178e-02,  7.4036e-02, -9.4656e-02,\n",
       "        -2.9554e-03,  7.2100e-02, -1.3796e-02, -7.3376e-02,  1.3651e-03,\n",
       "         2.2423e-01,  2.5347e-01, -2.9496e-01,  1.7043e-02,  1.9483e-01,\n",
       "         4.0727e-02, -1.8151e-01,  6.8319e-02, -1.1627e-01,  1.3987e-01,\n",
       "        -1.2464e-01,  1.6415e-01,  1.1659e-01, -2.9262e-01, -1.3138e-01,\n",
       "         2.8386e-01,  1.3536e-02,  1.7892e-02,  1.2224e-01, -8.3474e-02,\n",
       "        -1.8547e-01,  1.3922e-01, -6.6026e-02,  1.8780e-01,  9.8016e-02,\n",
       "         2.2783e-01,  1.8879e-01, -1.1789e-01,  8.7847e-02,  9.5319e-02,\n",
       "        -1.3631e-01, -1.2319e-01, -5.0856e-02,  1.7521e-01, -8.8601e-03,\n",
       "        -2.1211e-01, -1.6603e-01, -9.1632e-02, -1.8940e-01,  1.2483e-01,\n",
       "        -3.4137e-01,  1.7253e-01, -4.0489e-03, -2.5830e-01,  3.4354e-01,\n",
       "         7.2385e-02,  6.4228e-02, -1.0345e-01,  8.1470e-02,  1.2823e-01,\n",
       "         3.0450e-01,  1.2547e-02,  9.2170e-02,  7.6265e-02,  1.9233e-01,\n",
       "         2.5052e-02,  1.6779e-01, -1.1386e-01, -1.2343e-01,  2.1583e-01,\n",
       "         5.1995e-02, -1.1094e-01, -1.3655e-01, -2.2206e-02, -4.0816e-02,\n",
       "        -1.5961e-01, -2.5930e-01, -3.9237e-02,  1.4823e-01,  1.7264e-01,\n",
       "        -1.0341e-01, -7.3078e-02,  5.3901e-03,  4.6488e-02, -5.2855e-02,\n",
       "         2.3029e-01,  1.3836e-01, -2.3104e-01,  5.1622e-02, -2.5691e-02,\n",
       "        -7.3428e-02, -2.0090e-01,  8.6826e+00, -1.3597e-02, -1.5189e-01,\n",
       "        -2.2824e-02,  1.5231e-01, -1.7320e-01, -6.9212e-02,  1.3925e-02,\n",
       "         3.7047e-01,  6.3549e-01, -1.1657e-01,  2.5321e-01, -3.9477e-01,\n",
       "         2.1688e-02,  4.9023e-02, -1.9004e-01,  1.0255e-01, -3.4117e+00,\n",
       "        -1.6341e-02,  1.7833e-01, -2.9481e-03,  2.0511e-01,  2.2708e-02,\n",
       "        -1.5878e-01,  4.0415e-01,  2.8988e-01,  1.8286e-01, -8.9413e-02,\n",
       "         1.9508e-01, -2.8454e-01, -3.6216e-01, -2.0818e-01,  1.3445e-01,\n",
       "         1.0488e-01,  1.0607e-01, -4.8979e-02,  3.1894e-01,  4.3418e-02,\n",
       "        -9.3010e-02, -1.1065e-01,  6.5195e-02,  1.7126e-01,  6.9784e-03,\n",
       "         4.1325e-01,  2.0223e-01,  1.0159e-01, -2.1637e-03,  1.3869e-01,\n",
       "        -2.3206e-01,  1.6523e-02,  4.7522e-01,  3.5994e-01, -4.1203e-01,\n",
       "        -3.4080e-02, -1.8557e-01,  2.1812e-01, -1.1671e-01, -1.9644e-01,\n",
       "         3.0313e-02,  7.6662e-02, -1.4767e-01,  3.0242e-01, -3.8838e-01,\n",
       "         1.2630e-01,  8.6338e-04, -2.1858e-01, -3.0865e-01, -8.5570e-02,\n",
       "         4.7794e-03, -1.4999e-01,  1.9752e-02,  1.9733e-01,  8.4233e-02,\n",
       "        -1.1645e-01, -1.5909e-01, -4.3136e-02, -1.8887e-03, -1.1042e-01,\n",
       "        -8.6980e-02, -2.1983e-01, -2.5740e-02,  6.9247e-02, -1.7009e-01,\n",
       "         2.6890e-01, -3.8613e-01, -2.2737e-01,  7.8148e-02, -4.8904e-02,\n",
       "        -1.5376e-01, -1.6890e-01,  9.6795e-02, -9.0670e-02, -1.9517e-02,\n",
       "         1.0524e-01,  1.8782e-01,  3.2855e-02,  1.2531e-01, -1.3895e-02,\n",
       "        -1.0850e-02,  7.8530e-02,  1.2075e-01,  1.0289e-01,  9.0531e-02,\n",
       "        -1.0503e-01, -1.5542e-02, -1.0592e-01, -5.9506e-02,  4.2483e-01,\n",
       "        -3.3485e-01, -1.1832e-01, -1.0529e-01,  1.4116e-01,  7.1300e-02,\n",
       "         2.0267e-01,  1.3006e-01, -4.7152e-02, -1.9340e-01,  9.9480e-02,\n",
       "         3.2894e-01, -1.5424e-01, -2.4012e-01, -1.7370e-01,  1.0893e-02,\n",
       "         3.6012e-03, -1.7056e-01, -4.7541e-02,  9.2827e-02, -1.1986e-01,\n",
       "         9.8332e-03,  1.3779e-02, -1.0578e-01, -1.5546e-04, -1.4863e-02,\n",
       "         8.7218e-02, -1.3129e-01, -1.6176e-01,  1.8377e-01, -3.2393e-01,\n",
       "        -6.3812e-02,  1.1426e-01, -3.7811e-01,  1.4901e-01,  1.6303e-01,\n",
       "        -8.0644e-02, -2.5430e-02,  1.3871e-01, -2.2643e-01, -9.5727e-02,\n",
       "        -3.8660e-02, -1.0742e-01, -2.5748e-01,  3.4333e-03,  1.8189e-01,\n",
       "        -4.2146e-02,  9.7471e-03,  4.0985e-02, -1.6121e-01,  1.8116e-01,\n",
       "         9.7045e-02,  1.7077e-01,  4.1108e-01, -3.9961e-01,  3.8703e-02,\n",
       "         8.6713e-02,  2.4180e-01,  1.3050e-01, -2.0577e-01,  2.3869e-01,\n",
       "         7.2552e-02, -9.9135e-01, -2.5906e-02,  3.1244e-01, -9.2438e-02,\n",
       "        -3.2326e-02,  2.2206e-01,  3.5055e-01,  2.9333e-01, -5.4146e-02,\n",
       "        -1.0362e-01,  2.1374e-02,  5.3769e-02, -1.9133e-01,  3.8761e-02,\n",
       "        -1.2608e-01,  2.3885e-01, -2.3160e-01,  3.1447e-01,  2.7911e-01,\n",
       "        -3.7621e-02, -2.3407e-01,  4.1730e-02,  5.6719e-02, -1.3127e-02,\n",
       "         1.8311e-01,  6.9706e-03,  1.2088e-01, -2.6926e-01, -4.1643e-02,\n",
       "         6.8666e-02,  3.4110e-01])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.predict('this is a test', one=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb08cc",
   "metadata": {},
   "source": [
    "Similar procedure with the visual part, which takes `PIL.Image` instances as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc0f0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_model = TorchModel(\n",
    "    identifier='clip_image',\n",
    "    preprocess=preprocess,\n",
    "    object=model.visual,\n",
    "    encoder=my_tensor_encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e3a7685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0578e-02, -1.6221e-01, -5.5376e-02,  7.8390e-02, -1.2022e-01,\n",
       "         5.0640e-01,  1.0211e-01,  2.2622e-01,  3.2397e-01,  6.2261e-02,\n",
       "         3.5964e-01,  2.9982e-01,  6.0377e-02,  8.5026e-02, -5.3123e-01,\n",
       "         1.3775e-01,  3.2034e-01,  4.3730e-01,  1.9380e-01, -1.1651e-01,\n",
       "        -1.3150e-01, -5.1755e-01,  6.1077e-01, -2.2255e-01, -7.0276e-01,\n",
       "         3.0484e-01,  6.1931e-02, -5.8035e-01,  2.2770e-01, -6.4251e-01,\n",
       "         3.3999e-01, -1.7397e-02,  7.0908e-02, -9.2297e-02, -7.9436e-02,\n",
       "        -3.1447e-01, -3.1228e-01,  2.3822e-02, -5.0216e-01,  1.4930e+00,\n",
       "         8.4230e-02, -2.3044e-02, -1.1477e-01,  3.5502e-02, -1.2837e-01,\n",
       "        -9.5825e-01, -5.2861e-01,  1.8252e-01,  6.3224e-01, -3.9892e-01,\n",
       "         4.7636e-01, -2.0618e-01,  1.2273e-01,  3.8639e-01, -1.0503e-01,\n",
       "         3.0455e-01, -2.9782e-02,  3.6526e-02, -2.8694e-01,  3.2250e-01,\n",
       "         5.4747e-01, -2.3506e-01, -9.4274e-02, -2.6075e-01, -2.7888e-01,\n",
       "         1.8752e-01, -8.9015e-02, -3.2810e-01, -3.7667e-01, -2.2993e-01,\n",
       "        -3.8283e-01,  1.5670e-01, -2.0009e-01, -6.1516e-01, -4.6129e-01,\n",
       "        -2.3686e-01, -1.0059e-02, -7.9703e-02, -3.4821e-01,  3.1059e-01,\n",
       "        -1.7159e-01, -2.9298e-01, -2.4381e-01, -2.5856e-01,  7.0778e-01,\n",
       "         1.2242e-02,  2.2205e-02, -3.0967e-01, -4.0190e-01, -2.6353e-02,\n",
       "         1.6317e-02, -1.9972e-01, -7.4539e+00,  5.8628e-01,  2.2497e-01,\n",
       "         4.1830e-01, -2.0617e-01, -5.7479e-01, -8.2537e-01,  3.1005e-01,\n",
       "        -3.4798e-01, -5.2859e-01,  2.6467e-01,  1.9279e-02, -2.2802e-01,\n",
       "         1.9532e-01,  4.1350e-02,  1.8429e-01, -1.8407e-02, -1.8370e-01,\n",
       "         2.8223e-02,  6.6122e-01, -5.5858e-02,  2.3884e-01,  1.1940e-01,\n",
       "         1.3521e-01, -1.1441e-01, -8.1836e-02, -7.4636e-02,  4.2051e-01,\n",
       "        -1.0685e-02, -4.8992e-01, -3.4659e-01, -1.6895e-01, -2.1679e-01,\n",
       "         2.8090e-01,  1.8629e-01,  3.8019e-01,  2.8722e-01,  3.0216e-01,\n",
       "        -5.0571e-02, -1.7411e-01, -1.4874e-01,  9.0259e-01,  1.1910e-01,\n",
       "         3.6449e-01,  1.9206e-01,  9.9890e-02, -7.4075e-02,  1.0057e-01,\n",
       "         3.4364e-01, -4.4466e-02, -2.2049e-02, -3.1023e-01, -6.1562e-01,\n",
       "         2.0031e-01, -3.2694e-01,  2.6710e-01, -3.7718e-02, -1.6477e-03,\n",
       "         4.7076e-01, -2.4078e-01,  7.6262e-01, -3.5806e-02, -7.0064e-02,\n",
       "        -8.6227e-02, -1.5303e-01,  3.8171e-01, -1.0009e-02, -2.4757e-01,\n",
       "        -2.0626e-02,  3.5690e-02,  4.2816e-01,  2.1779e-01, -2.2666e-01,\n",
       "        -1.2751e-01, -4.8731e-03,  1.6475e-02,  3.7198e-01, -2.9066e-01,\n",
       "         2.3526e-01, -2.1536e-01,  2.2257e-01, -4.0267e-02,  4.8602e-01,\n",
       "        -2.0090e-01,  1.1203e+00, -1.3466e-01, -1.5637e-01, -3.7060e-01,\n",
       "         3.3234e-01,  2.7730e-01,  4.1123e-02,  5.4512e-01, -5.6766e-01,\n",
       "         1.8852e-01, -1.8356e-01, -1.8439e-01, -6.1105e-01, -9.8356e-02,\n",
       "        -9.4271e-02, -2.6262e-01, -5.4949e-01, -1.4423e-01,  6.4394e-01,\n",
       "         3.1086e-01,  5.6051e-01, -2.0735e-02,  4.9755e-01,  1.0045e-01,\n",
       "        -2.2530e-02, -2.2911e-01, -1.1739e-01, -8.2579e-02,  1.6977e-01,\n",
       "         5.1445e-01, -6.3668e-01, -3.6487e-01, -1.9777e-01,  3.9829e-01,\n",
       "        -3.2453e-01,  1.3883e+00, -1.3430e-01, -2.2352e-01, -4.4370e-02,\n",
       "        -1.4378e-02, -2.8561e-01,  4.4027e-01,  5.1375e-01,  1.9691e-01,\n",
       "         4.4492e-01, -2.6746e-01, -3.3590e-03, -9.4953e-02,  6.4033e-01,\n",
       "         3.2501e-01,  2.0999e-01,  2.3202e-01, -6.1609e-01, -1.1878e-01,\n",
       "         5.5008e-01,  1.6608e-02, -5.7188e-02, -2.7516e-01, -3.6895e-01,\n",
       "         6.3370e-01,  8.7343e-02, -4.3381e-01,  1.5305e-01, -6.7085e-02,\n",
       "         5.6645e-02, -1.1391e-02,  1.6145e-02, -1.8845e-01,  2.3812e-01,\n",
       "        -7.1040e-01, -4.7566e-01,  4.9583e-01,  1.2471e-01,  1.5216e-01,\n",
       "         1.4284e-01, -1.0380e-01,  1.6335e-01, -7.4495e-02, -1.2739e-02,\n",
       "         6.5095e-01, -1.8981e-01, -2.8833e-01, -7.3252e-01, -5.9771e-01,\n",
       "         2.1111e-01,  7.0241e-02, -8.3747e-02,  3.7308e-01,  1.9920e-01,\n",
       "         1.3902e-02, -1.3614e-01, -1.7695e-01,  2.0503e-01,  9.3711e-02,\n",
       "         8.0148e-02,  8.5672e-02, -2.8120e-01, -6.7461e-03, -2.1041e-01,\n",
       "         2.1493e-01,  1.8493e-01, -4.4218e-01, -4.7837e-01,  1.6753e-01,\n",
       "        -2.3293e-02,  3.5159e-01, -3.1465e-01, -5.4347e-02, -4.0912e-01,\n",
       "        -2.2053e-01,  9.5728e-01, -3.9913e-01, -1.4328e-01,  1.3221e-01,\n",
       "         1.2045e-01,  4.2445e-01, -3.5934e-01, -4.8063e-02, -4.6057e-01,\n",
       "         2.9698e-01, -6.9516e-02, -1.3217e-01,  1.4138e-01,  3.1053e-01,\n",
       "        -3.7533e-01, -3.8831e-01,  2.6178e-01,  3.1733e-01, -6.4162e-01,\n",
       "        -1.0972e-01,  3.6213e-02, -3.5652e-01,  5.0321e-01,  1.7369e-01,\n",
       "        -3.4125e-01,  6.4234e-01,  9.0110e-01,  4.1672e-01, -4.9290e-02,\n",
       "        -2.4962e-01,  4.3435e-01,  4.3039e-01, -6.6409e-02, -1.5867e-01,\n",
       "        -2.1911e-01,  2.4160e+00, -3.5952e-01, -8.0127e-01,  2.8247e-01,\n",
       "         2.8451e-01,  2.8281e-01, -1.3663e-01, -1.1931e-01,  3.6793e-01,\n",
       "         4.1910e-02,  3.1490e-01,  1.9783e-01, -3.7943e-01,  5.2497e-02,\n",
       "        -2.5192e-01, -1.7912e-01,  2.3607e-01,  7.3867e-01, -5.4121e-01,\n",
       "        -2.1517e-01,  1.6668e-01,  2.2764e-02,  2.8820e-01, -3.1615e-01,\n",
       "         3.1915e-01,  2.1860e-01,  1.4942e-01,  8.4099e-03,  5.3017e-01,\n",
       "         6.0099e-01, -1.4114e-01, -1.4856e-01, -1.6554e-01,  2.5043e-01,\n",
       "         2.7558e-02, -1.0879e-01,  9.0948e-01,  2.0359e-02, -3.8184e-01,\n",
       "         1.1412e+00, -4.0500e-02,  2.4100e-01, -9.1100e-02, -7.0314e-01,\n",
       "        -1.7057e-01, -2.4310e-01,  3.2672e-01, -2.3618e-01, -9.0233e-02,\n",
       "        -4.9724e-02, -8.6624e-02,  5.6699e-01,  2.8881e-01, -8.2658e-02,\n",
       "        -8.8262e-04,  8.4864e-01,  7.0677e-02,  2.3031e-02, -2.4866e-01,\n",
       "         1.1641e-01, -2.8474e-01, -1.3177e-01, -2.2425e-03, -1.2801e-01,\n",
       "         2.7398e-01,  4.2915e-01,  8.8775e-01, -4.8160e-01,  1.1280e+00,\n",
       "        -8.4414e-01, -6.9511e-01,  1.6730e-01, -2.0220e-01, -7.0196e-01,\n",
       "         1.7196e-01, -2.8753e-01, -3.3573e-01,  3.1275e-01,  3.6272e-01,\n",
       "         2.6284e-01,  2.8220e-01,  9.7478e-01, -2.0235e-01, -2.1128e-01,\n",
       "        -3.9525e-02, -2.9955e-01,  1.4826e-01,  5.9088e-01,  3.6298e-01,\n",
       "         1.0216e-01,  3.6746e-01,  1.5965e-01, -7.3041e-02, -1.6660e-01,\n",
       "         3.2255e-01, -2.6789e-01,  2.2527e-01, -5.5620e-01, -4.7218e-01,\n",
       "        -2.4462e-01, -1.8148e-01, -1.5251e-01, -2.2246e-01, -1.4241e-01,\n",
       "         4.3461e-01,  5.2278e-01, -7.4600e-01, -1.7403e+00, -6.6919e-02,\n",
       "         3.0356e-01, -1.0897e-01,  1.3690e+00, -3.1331e-01,  6.4572e-02,\n",
       "        -9.3127e-02,  3.5798e-01,  1.7314e-01,  4.8131e-01, -1.0321e-02,\n",
       "        -8.1382e-02,  2.7095e-01,  3.9074e-01,  2.8987e-01, -1.7033e-01,\n",
       "         1.6459e-01,  3.3957e-01, -1.9293e-01, -1.0173e-01,  2.8447e-01,\n",
       "        -1.0885e-01,  1.1944e-01, -1.3210e-01,  1.3350e-01, -5.3832e-02,\n",
       "        -2.1604e-01, -5.0783e-01,  1.2194e-01,  4.9723e-01,  2.9972e-02,\n",
       "         3.2896e-01, -3.6968e-01, -1.5322e-02, -2.7205e-01,  5.9543e-01,\n",
       "         3.8685e-02,  3.8294e-01, -2.7127e-02, -2.1152e-01, -1.1695e-01,\n",
       "         4.5063e-01,  1.0655e-01,  1.6503e-01, -5.0032e-01,  2.8248e-03,\n",
       "        -3.9469e-01, -1.9241e-01,  2.3627e-01,  6.7822e-02, -7.0755e-02,\n",
       "        -2.3076e-02, -2.2183e-02,  2.0974e-01,  6.3637e-03,  1.7290e-01,\n",
       "         2.1337e-01, -8.9225e-03,  8.0182e-02,  5.4698e-03, -3.8995e-01,\n",
       "         2.2839e-01, -7.6282e-02,  7.0927e-02, -1.3024e-01, -1.0116e+00,\n",
       "        -1.8865e-01,  6.5041e-02, -4.1888e-01,  3.3341e-01,  5.5684e-02,\n",
       "        -5.6362e-02, -1.2702e-01,  2.0550e-01,  3.5279e-02,  2.3319e-01,\n",
       "        -9.0900e-02,  2.9851e-01,  1.0577e-01,  1.3509e-01,  7.0622e-01,\n",
       "        -2.6421e-01,  4.6757e-01])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_model.predict(x, one=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716bcb2",
   "metadata": {},
   "source": [
    "Now let's create the index for searching by vector. We register both models with the index simultaneously,\n",
    "but specifying that it's the `visual_model` which will be responsible for creating the vectors in the database\n",
    "(`indexing_watcher`). The `compatible_watcher` specifies how one can use an alternative model to search \n",
    "the vectors. By using models which expect different types of index, we can implement multimodal search\n",
    "without further ado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74f607e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:model/clip_image/0 already exists - doing nothing\n",
      "WARNING:root:model/clip_image/0 already exists - doing nothing\n",
      "WARNING:root:model/clip_image/0 already exists - doing nothing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing chunk 0/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2906/2906 [01:46<00:00, 27.39it/s]\n",
      "WARNING:root:encoder/torch.float32[512]/0 already exists - doing nothing\n",
      "INFO:root:loading hashes: 'my-index'\n",
      "/Users/levkonstantinovskiy/Documents/GitHub/new/superduperdb-stealth/superduperdb/encoders/torch/tensor.py:25: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:212.)\n",
      "  return torch.from_numpy(array)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from superduperdb.core.vector_index import VectorIndex\n",
    "from superduperdb.core.watcher import Watcher\n",
    "\n",
    "db.add(\n",
    "    VectorIndex(\n",
    "        'my-index',\n",
    "        indexing_watcher=Watcher(\n",
    "            model=visual_model,\n",
    "            key='img',\n",
    "            select=collection.find(),\n",
    "        ),\n",
    "        compatible_watcher=Watcher(\n",
    "            model=text_model,\n",
    "            key='text',\n",
    "            active=False,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18971a6d",
   "metadata": {},
   "source": [
    "We can now demonstrate searching by text for images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab994b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_query = {\n",
    "        \n",
    "        'Category': 'Footwear',\n",
    "        'ProductType': 'Tops',\n",
    "  }\n",
    "out = db.execute(\n",
    "    collection.like(SuperDuperDocument({'text': 'running shoes'}), vector_index='my-index', n=3).find(filter_query)\n",
    ")\n",
    "for r in out:\n",
    "    display(r['img'].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4b9424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _extract(out):\n",
    "    return [(row['img'].x) for row in out]\n",
    "\n",
    "def query_db(text, category, sub_category, product_type, colour, usage):\n",
    "    filter_query = {}\n",
    "    all_label = 'All'\n",
    "    populated =  lambda x: (not x==all_label) and x\n",
    "    if populated(category):\n",
    "        filter_query['Category'] = category\n",
    "    if populated(sub_category):\n",
    "        filter_query['SubCategory'] = sub_category\n",
    "    if populated(product_type):\n",
    "        filter_query['ProductType'] = product_type\n",
    "    if populated(all_label):\n",
    "        filter_query['Colour'] = colour\n",
    "    if populated(all_label):\n",
    "        filter_query['Usage'] = usage\n",
    "\n",
    "    code = f\"\"\"\n",
    "    import clip \n",
    "    import pymongo\n",
    "    \n",
    "    from superduperdb.misc.superduper import superduper\n",
    "    from superduperdb.datalayer.mongodb.query import Collection\n",
    "    from superduperdb.core.document import Document as D\n",
    "    \n",
    "\n",
    "    db = pymongo.MongoClient().documents\n",
    "    db = superduper(db)\n",
    "\n",
    "    collection = Collection(name='tiny-imagenet')\n",
    "\n",
    "    cursor = db.execute(collection.like(D'{'text:{text}'}'), vector_index='{'my-index'}', n=20).find({filter_query})\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    similar_doc = SuperDuperDocument({'text':text})    \n",
    "    cursor = collection.like(similar_doc, vector_index='my-index', n=20).find(filter_query)\n",
    "    results = db.execute(cursor)\n",
    "    return (_extract(results), code)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8af868e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5k/k58wnkmx6x1cdgy02s9v8hnr0000gn/T/ipykernel_78227/573181240.py:23: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  sub_category = gr.inputs.Dropdown(sub_category_values, label='SubCategory')\n",
      "/var/folders/5k/k58wnkmx6x1cdgy02s9v8hnr0000gn/T/ipykernel_78227/573181240.py:23: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  sub_category = gr.inputs.Dropdown(sub_category_values, label='SubCategory')\n",
      "/var/folders/5k/k58wnkmx6x1cdgy02s9v8hnr0000gn/T/ipykernel_78227/573181240.py:24: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  product_type = gr.inputs.Dropdown(product_type_values, label='ProductType')\n",
      "/var/folders/5k/k58wnkmx6x1cdgy02s9v8hnr0000gn/T/ipykernel_78227/573181240.py:24: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  product_type = gr.inputs.Dropdown(product_type_values, label='ProductType')\n",
      "/var/folders/5k/k58wnkmx6x1cdgy02s9v8hnr0000gn/T/ipykernel_78227/573181240.py:25: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  colour = gr.inputs.Dropdown(colour_values, label='Colour')\n",
      "/var/folders/5k/k58wnkmx6x1cdgy02s9v8hnr0000gn/T/ipykernel_78227/573181240.py:25: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  colour = gr.inputs.Dropdown(colour_values, label='Colour')\n",
      "/var/folders/5k/k58wnkmx6x1cdgy02s9v8hnr0000gn/T/ipykernel_78227/573181240.py:26: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  usage = gr.inputs.Dropdown(usage_values, label='Usage')\n",
      "/var/folders/5k/k58wnkmx6x1cdgy02s9v8hnr0000gn/T/ipykernel_78227/573181240.py:26: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  usage = gr.inputs.Dropdown(usage_values, label='Usage')\n",
      "/var/folders/5k/k58wnkmx6x1cdgy02s9v8hnr0000gn/T/ipykernel_78227/573181240.py:34: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "  openai_gallery = gr.Gallery(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def get_unique_values(column_name, df=df):\n",
    "    return  ['All'] + df[column_name].unique().tolist() \n",
    "\n",
    "\n",
    "\n",
    "category_values = get_unique_values('Category')\n",
    "sub_category_values = get_unique_values('SubCategory')\n",
    "product_type_values = get_unique_values('ProductType')\n",
    "colour_values = get_unique_values('Colour')\n",
    "usage_values = get_unique_values('Usage')\n",
    "\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tab(\"Multi-modal search\"):\n",
    "        with gr.Row():\n",
    "            \n",
    "            text_query = gr.Textbox(value=\"Heels\", label='Search query', show_label=True)\n",
    "            \n",
    "            category = gr.Dropdown(category_values, label='Category')\n",
    "            sub_category = gr.inputs.Dropdown(sub_category_values, label='SubCategory')\n",
    "            product_type = gr.inputs.Dropdown(product_type_values, label='ProductType')\n",
    "            colour = gr.inputs.Dropdown(colour_values, label='Colour')\n",
    "            usage = gr.inputs.Dropdown(usage_values, label='Usage')\n",
    "            \n",
    "        with gr.Row():\n",
    "            b2 = gr.Button(\"Submit\")\n",
    "        with gr.Row():\n",
    "            code = gr.Code(label=\"Code\", language=\"python\")\n",
    "        with gr.Row():\n",
    "            \n",
    "            openai_gallery = gr.Gallery(\n",
    "                    label=\"Similar images from CLIP\", show_label=True, elem_id=\"gallery\"\n",
    "                ).style(columns=[6], rows=[3], object_fit=\"contain\", height=\"auto\")\n",
    "\n",
    "            \n",
    "        b2.click(query_db, inputs=[text_query, category, sub_category, product_type, colour, usage], outputs=[openai_gallery,  code])\n",
    "        \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0d514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba866f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
